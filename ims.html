<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>IMS</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@400;700&family=Poppins:wght@400;700&family=Manrope:wght@400;500;700;800&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="./css/ims.css">
    
</head>
<body>
  <div class="topnav">
    <div class="hamburger" onclick="toggleMenu()">â˜°</div>
    <a href="index.html">Home</a>
    <a href="index.html#works">Works</a>
    <a href="https://github.com/jihyeon26">Github</a>
    <a href="about.html">About</a>
    <a href="artworks.html">Artworks</a>
  </div>
  <header>
      <div class="container">
          <p class="project-tag">IMS</p>
          <h1>Weaving Your Travel Moments into Stories</h1>
      </div>
  </header>

  <div class="image-link">
    <div class="image-container">
      <div class="gradient-circle" style="background-color: rgb(227, 181, 177);"></div>
      <img class="cover" src="./assets/ims/ims_door.png" style="width: 80%; height: auto;display: block; margin:0 auto;" alt="IMS Project">                  
    </div>
  </div>

  <section class="overview">
      <div class="container">
        <h2>Overview</h2>
        <p>
          IMS (Image to Story) is an AI-powered platform designed to turn your travel photos into compelling narratives. By analyzing metadata like location, time, and additional user-provided inputs, IMS generates personalized and emotional travel stories, providing a unique way to preserve and share your memories.
          <br>This project was awarded 1st place among the 10 teams in the second project phase of Microsoft AI School.
        </p>
        <img src="./assets/ims/ims_gif0.gif" style="width: 70%; border: 1px solid #d4d4d4; border-radius: 10px; margin:0 auto; margin-top: 40px;display: block;">
    </div>
  </section>

  <section class="role">
      <div class="container">
          <h2>My role</h2>
          <p>I led the development of the core OpenAI integration, designed the overall program structure, and created prototypes to refine and concretize the service. I also defined user input parameters and conducted extensive testing and optimization to improve the system's generation performance.</p>
          <ul>
            <li><strong>Me: </strong>Machine Learning Lead, UI/UX Designer</li>
            <li><strong>Geonjin Jung:</strong> Team Leader</li>
            <li><strong>Daegeon Lim:</strong> Project Manager</li>
            <li><strong>Jaei Lee:</strong> Project Management Office</li>
            <li><strong>Hoyoung Kim:</strong> Frontend Lead.</li>
            <li><strong>Euijin Ahn:</strong> Technical/Backend Lead.</li>
            <li><strong>Hyunyoung Lee:</strong> Data Engineer.</li>
            <li><strong>Sumin Kim:</strong> Machine Learning Engineer.</li>
            <li><strong>Hyundong Lee:</strong> Service Planner/QA.</li>
          </ul>
      </div>
  </section>

  <section class="details">
      <div class="container-target">
          <div class="detail-item">
              <h3>Target users</h3>                              
                  <p>Travelers looking for an effortless way to document and share their journeys.</p>              
          </div>
          <div class="detail-item">
              <h3>Interface</h3>
              <p>Website</p>
          </div>
          <div class="detail-item">
              <h3>Timeline</h3>
              <p>2 weeks</p>
          </div>
      </div>
  </section>

  <section class="business-opportunity">
    <div class="container">
      <h2>Key Features</h2>
      <ul>
        <li><strong>Automatic Story Generation:</strong> Upload photos, and our AI crafts a travel narrative.</li>
        <li><strong>Location-Aware Narratives:</strong> Stories leverage photo metadata and image recognition to identify locations and landmarks.
        <li><strong>Easy Sharing:</strong> Save your stories in blog format and share them with friends and family.</li>
      </ul>
    </div>
  </section>

  <section class="problems-section">
      <div class="container2">
        <h2>Problems Solving</h2> 

        <div class="problem">
          <h3>Problem 1</h3>
          <p>When users upload only images, the system sometimes generates stories with incorrect or irrelevant locations, creating a disconnect between the userâ€™s actual experience and the narrative.</p>
          <p><span class="solution">ðŸ’¡ Solution:</span> To ensure accuracy, we extract EXIF metadata from photos to retrieve precise location data. Additionally, we utilize image recognition to identify landmarks, allowing the system to construct a story grounded in the user's real journey.</p>
        </div>

        <div class="problem">
          <h3>Problem 2</h3>
          <p>Without additional context, the system occasionally describes individuals in photos using generic terms, such as "a woman," leading to impersonal and awkward storytelling.</p>
          <p><span class="solution">ðŸ’¡ Solution:</span> By enabling users to input details such as gender and information about their companions, we enrich the narrative with meaningful context, transforming descriptions into more personalized and relatable stories.</p>
        </div>

        <div class="problem">
          <h3>Problem 3</h3>
          <p>Many users find it burdensome to sort through their travel photos and write detailed accounts of their experiences, leading to missed opportunities to preserve their memories.</p>
          <p><span class="solution">ðŸ’¡ Solution:</span> Our platform minimizes user effort by automating the story creation process. With minimal input, the system crafts a cohesive and emotional travel narrative, allowing users to focus on reliving their memories rather than organizing them.</p>
        </div>          
      </div>
  </section>

  <section class="data-process">
    <div class="container">
        
        <h2>Prototype</h2>
        <img src="./assets/ims/prototype.png" style="width: 100%; margin: 40px 0px;">
        <img src="./assets/ims/prototype_code.png" style="width: 80%; max-width:600px; display: block; margin: auto;">
        <p>The initial prototype used the PIL and Geopy libraries to convert GPS data into readable addresses. For better performance, the system was later enhanced in the areas of location data, computer vision, and OpenAI. Details are provided below.</p>

        <br><br>


        <h2>Improvements</h2>
        <p>
          <img src="./assets/ims/model.png" style="width: 100%; margin: 40px 0px;">
        </p>
        

        <h3>1. Extract location (Azure Map API, Azure Computer Vision, Google Cloud Vision)</h3>
        <p> <br>
            Step 1 - Azure Maps: Extract GPS metadata from the image's EXIF data to identify landmarks, cities, and countries.
            <br><br>
            Step 2 - Azure Computer Vision: Perform landmark detection. If successful, the process ends here. While highly accurate for detected landmarks, the scope of recognizable landmarks is limited. If detection fails, proceed to Step 3.
            <br><br>
            Step 3 - Google Cloud Vision: Attempt landmark detection. This tool recognizes a wide range of landmarks but sometimes provides overly detailed or inaccurate results.
        </p>
        <img src="./assets/ims/maps.png" style="width: 100%; margin-bottom:60px">
        <p>
          Azure Computer Vision demonstrated lower recognition rates, but when it did identify landmarks, the accuracy was significantly higher. On the other hand, Google Cloud Vision showed higher recognition rates but often lacked precision, occasionally providing overly detailed or irrelevant locations. Therefore, we decided to prioritize recognition using Azure Computer Vision first, and if it fails, supplement the results with Google Cloud Vision for better coverage.
        </p>
        <img src="./assets/ims/maps2.png" style="width: 100%; margin-bottom:60px">
      
        


        <h3>2. Generate Caption (Azure Computer Vision)</h3>
        <p>Images are uploaded and analyzed using the Azure Computer Vision API via Azure Portal. The API generates dense captions along with confidence scores for each caption.
        Images are resized to a maximum of 1290x1080 for efficient processing, and dense captions with confidence scores above 0.5 are filtered. The processing time is calculated, and data is organized.
        </p>
        <img src="./assets/ims/caption.png" style="width: 100%; margin-bottom:30px">
        <p>
          We explored various methods to detect landmarks in photos using computer vision. As a result, we discovered that extracting a combination of Description, Categories, and Tags enables the recognition of iconic landmarks like the Eiffel Tower, unlike when extracting only the Description.
        </p>
        
        <img src="./assets/ims/vision.png" style="width: 50%; margin-bottom:60px">

        <h3>3. Generate Story (Azure OpenAI)</h3>
        <p>
          Captions and location data are processed by Azure OpenAI to generate travel stories, enhanced by user-provided parameters to improve model performance and ensure personalized, contextually relevant narratives.
        </p>
        <img src="./assets/ims/parameter.png" style="width: 100%;">

          <p>
          For the gender parameter, we initially considered using Azure Face for facial detection, but after receiving confirmation that it was not available for use(Face service is only available to Microsoft managed customers and partners), we added this parameter as an alternative to address the limitation.
          </p>
            After add gender parameter, "A woman stands gazing at an old door" becomes "I stood before the old door, lost in its weathered charm."
            <br>
            Companions add shared experiences to the story. For instance, "Two women admire a distant mountain range" transforms into "My sister and I stood atop the rocky hill, marveling at the distant peaks."
            
        </p>
        <img src="./assets/ims/parameter2.png" style="width: 100%;">


    </div>
</section>

</section>
<section class="UI design">
  <div class="container">
      <h2>UI Design</h2><br>
      <p>
        The Service UI is designed with a focus on making it easy for users to use. 
        During the development process, the pages and interaction flow were structured with consideration for the backend functionality. Some design elements were adjusted during the frontend implementation. 
      </p>
      <img src="./assets/ims/ims_ui.png" style="width: 100%; border-radius: 5px; margin:0 auto; margin-top: 40px;display: block;">
      <video controls style="width:60%; display: inline-block; margin-top:20px; border: 1px solid #ccc; border-radius: 5px; box-shadow: none;">
        <source src="./assets/ims/ui_fin.mp4" type="video/mp4">
        Your browser does not support the video tag.
    </video>
        
  </div>
</section>

<section class="tools">
  <div class="container">
      <h2>Tools Used</h2>
      <h3>UI/UX Frontend</h3>
      <ul>
          <li>Figma</li>
          <li>TypeScript & JavaScript</li>
          <li>React</li>
      </ul>

      <h3>Backend</h3>
      <ul>
          <li>GORM & Fiber (Go)</li>
          <li>Azure Database</li>
      </ul>

      <h3>Machine Learning and AI</h3>
      <ul>
          <li>Azure Maps, Google Cloud maps</li>
          <li>Azure Computer Vision</li>
          <li>Azure OpenAI</li>
      </ul>

      <h3>Infrastructure</h3>
      <ul>
          <li>Azure Blob Storage</li>
          <li>Azure VM</li>
          <li>Azure Database</li>
      </ul>
  </div>
</section>




<div id="footer" class="section">
    <div class="container">
      <p>          
        Copyright 2024. Jihyeon Choung All rights reserved.
      </p>
    </div>
</div>
        
<script src="./javascript/machuilin.js"></script>
</body>

</html>